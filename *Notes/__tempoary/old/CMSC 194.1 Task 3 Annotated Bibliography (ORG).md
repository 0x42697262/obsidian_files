Ghunaim, H. & Dichter, J. (2016). The significance of parameters’ optimization in fair benchmarking of software defects’ prediction performances. *International Journal of Advanced Research in Computer Science*, Volume 7, No. 1, January-February 2016. ISSN No. 0976-5697.

Ghunaim and Dichter proposes a research study that implements parameter's optimization as a research standard to have more reliable and valid software engineering research outcomes since prediction research is facing serious challenges to their reliability and validity because many published research outcomes contradict each other due to lack of research standard. Three prediction algorithms will be used in the analysis: Support Vector Machine (SVM), Multilayer Perceptron (MLP), and Naïve Bayesian (NB) with the help of KNIME as a data mining platform. The results provides two folds: It optimizes the performance of prediction models, and hyperparameter tuning provides a fair comparison of various prediction models, making it easier for practitioners to select the most suitable model for a given task.

The study provides a valuable contribution to the field of software engineering by highlighting the importance of properly optimizing parameters in order to accurately compare the performance of different prediction models. However, the study has some limitations that should be considered such as focus on one problem, limited number of models, and lack of detailed analysis of the results.

This paper would be beneficial to researchers heavily focused on optimizations however this does not relate to my study.

---
Bucev, M., & Kunčak, V. (2022). Formally verified quite ok image format. In *Proceedings of the 22nd Conference on Formal Methods in Computer-Aided Design–FMCAD 2022* (No. CONF, pp. 343-348). TU Wien.

Bucev and Kunčak suggests a new algorithm for encoding and decoding image formats since lossless conversion is present everywhere and are generally more complex like PNG formats. QOI encoder and decoder are both single passes that only traverses the pixel bit once. There are multiple cases in encoding an image but generally it checks the current index of the RGB values and keeps the previous index which will be encoded into four different cases where it will be written into tagged chunks thereby uniquely identifiying the applied cases. It is then verified that the decoding is the inverse of encoding.

The QOI incorporates previous algorithms and is modified to suit its needs to which it can be called a hack. The authors should have stated the reasons why they are implementing the transpilation of code to C language and explain why C is being used instead of other languages.

This study is not relevant to my research however I find it interesting and could potentially be used for my future research study.

---
Dinh, T. N., Xuan, Y., Thai, M. T., Park, E. K., & Znati, T. (2010, March). On approximation of new optimization methods for assessing network vulnerability. In _2010 Proceedings IEEE INFOCOM_ (pp. 1-9). IEEE.

Dinh, Xuan, Thai, Park, and Znati presents an investigation of new optimization methods for assessing network vulnerability, with a focus on approximation methods by discussing the importance of network vulnerability assessment and the current challenges in this area. The authors then introduce a new approach for approximating the optimal solution for network vulnerability assessment, using a combination of genetic algorithms and particle swarm optimization.

The authors provide a detailed description of the proposed method, including the mathematical formulation and the parameters used in the simulation. They then present the results of the simulation, which demonstrate the effectiveness of the proposed approach in approximating the optimal solution for network vulnerability assessment.

This paper is relevant to my area of research and could be useful in my study, potentially providing valuable insights and ideas for my research.